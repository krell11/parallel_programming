# Parallel Programming Course Project

## Постановка задачи: Решение 1D уравнения Пуассона

Рассматривается краевая задача для одномерного уравнения Пуассона на отрезке $[0, L]$ с однородными граничными условиями Дирихле:

$$-u''(x) = f(x), \quad x \in (0, L), \quad u(0) = u(L) = 0$$

При дискретизации методом конечных элементов (FEM) с использованием линейных базисных функций на равномерной сетке с шагом $h$, задача сводится к решению системы линейных алгебраических уравнений вида $Ax = b$. Матрица жесткости $A$ является трехдиагональной, и уравнение для $i$-го узла выглядит так:

$$\frac{2}{h} x_i - \frac{1}{h} x_{i-1} - \frac{1}{h} x_{i+1} = b_i$$

Где:
* $\text{diag} = \frac{2}{h}$ — диагональный элемент.
* $\text{off} = -\frac{1}{h}$ — внедиагональный элемент (влияние соседей).



### Метод Якоби
Для решения системы применяется итерационный метод Якоби. Выражая $x_i$, мы получаем формулу для обновления значения в узле на $(k+1)$-й итерации:

$$x_i^{(k+1)} = \frac{b_i - \text{off} \cdot x_{i-1}^{(k)} - \text{off} \cdot x_{i+1}^{(k)}}{\text{diag}}$$

**Обоснование параллелизма:**
Для вычисления нового значения $x_i^{(k+1)}$ используются только значения $x^{(k)}$ с **предыдущей** итерации. Это означает, что вычисления для всех узлов $i$ независимы друг от друга в рамках одного шага и могут выполняться одновременно.

## Реализация

Использовался метод **Domain Decomposition** (декомпозиция области), где мы делим изначальную область на равные отрезки и отдаем каждому вычислительному узлу свой отрезок.

Чтобы посчитать самую левую точку отрезка ($i$), нужно знать точку соседа слева ($i-1$), которая лежит в чужой памяти.

1.  **Shared Memory (Pthreads, OpenMP):**
    
    Память общая, поэтому данные соседа доступны напрямую. Проблема синхронизации решается с помощью **барьеров**. Все потоки ждут на барьере, пока завершится текущая итерация для всех узлов, и только после этого переходят к следующему шагу.

2.  **Distributed Memory (MPI):**
    
    Память раздельная. Проблема решается с помощью **явного обмена сообщениями**. Перед каждым шагом расчета процессы обмениваются значениями в крайних точках своих отрезков (Halo Exchange) с помощью `MPI_Sendrecv`.

## Результаты

![Графики производительности](https://github.com/user-attachments/assets/9db15517-dec1-4a2b-8b4d-be4057d09467)

### Анализ графика Speedup (Ускорение)

* **MPI_C (Красный):** Показал лучший результат (пик **~4.8x**). С — быстрый компилируемый язык, а MPI позволяет использовать ресурсы нескольких узлов кластера.
    * *Ограничение:* Задача слишком маленькая ($100k$ точек). После 48 процессов время на передачу данных по сети (коммуникация) становится больше, чем время самого расчета. Процессоры начинают простаивать в ожидании данных.
* **Pthreads (Зеленый):** Резкий старт (обгоняет OpenMP на малых потоках), но после 16 потоков производительность падает.
    * *Причина:* Высокие накладные расходы на барьеры (`pthread_barrier_wait`). Чем больше потоков, тем дольше они синхронизируются на каждом шаге.
* **OpenMP (Оранжевый):** Показывает стабильный, но умеренный рост с выходом на плато. Работает надежнее Pthreads, но ограничен ресурсами одного узла (Shared Memory) и пропускной способностью памяти.
* **MPI_Py (Синий):** Почти не ускоряется. Огромные накладные расходы интерпретатора Python и вызовы API MPI перекрывают время самих вычислений, делая решение такой "легкой" задачи неэффективным.

## Screencast
[Посмотреть запись работы](https://github.com/user-attachments/assets/122ff2b3-d618-4319-b24e-4eff6693becc)

## Вывод

В ходе работы была проанализирована эффективность параллельного решения 1D уравнения Пуассона методами OpenMP, Pthreads и MPI.

Наилучшую производительность продемонстрировала реализация **MPI на языке C**, достигнувшая пикового ускорения около **4.8x** благодаря компилируемой природе кода и отсутствию расходов на управление памятью интерпретатора. Однако на использованной размерности сетки ($10^5$ элементов) после 48 процессов наблюдается снижение ускорения, так как время на сетевую коммуникацию начинает преобладать над полезными вычислениями.

Сравнение методов для общей памяти показало преимущество **OpenMP** перед Pthreads, так как ручная реализация барьеров в Pthreads создает критические накладные расходы при большом числе потоков.

Версия на **Python**, несмотря на использование оптимизированных библиотек NumPy и mpi4py, показала минимальное ускорение, поскольку накладные расходы интерпретатора на организацию цикла и вызовы функций API перекрывают выигрыш от распараллеливания в задачах с большим количеством «легких» итераций.
