# parallel_programming
course project



# Постановка задачи: Решение задачи 1D Пуассона Методом Конечных Элементов
Реализация МКЭ через метод Якоби.
Так как для расчета точки $i$ нужны только старые значения соседей, мы можем обновлять все точки одновременно. Они не зависят друг от друга на текущем шаге.

# Реализация
Использовался метод Domain Decomposition, где мы делим изначальную область на равные отрезки и отдаем каждому процессу этот отрезок, чтобы посчитать самую левую точку отрезка $i+1$, нужно знать точку соседа слева. А она лежит в его памяти, это решается тем, что в Pthreads, OpenMP память общая поэтому можно использовать барьеры поэтому все потоки ждут пока досчитается итерация и только потом считаю дальше, в MPI память раздельная, но мы решаем эту проблему с помощью явного обмена сообщениями, перед каждым шагом расчета данные процессы обмениваются значениями в крайних точках своих отрезков
# Результаты


<img width="4500" height="1800" alt="final_graphs_v2" src="https://github.com/user-attachments/assets/9db15517-dec1-4a2b-8b4d-be4057d09467" />
График Speedup (Ускорение)MPI_C (Красный): Показал лучший результат (пик ~4.8x). С — быстрый компилируемый язык, а MPI позволяет использовать несколько узлов кластера.Задача слишком маленькая ($100k$ точек). После 48 процессов время на передачу данных по сети (коммуникация) становится больше, чем время самого расчета. Процессоры просто ждут данные.Pthreads (Зеленый): Резкий старт (обгоняет OpenMP на малых потоках), но после 16 потоков производительность падает, потому что высокие накладные расходы на барьеры. Чем больше потоков, тем дольше они ждут друг друга на каждом шаге.OpenMP (Оранжевый): Показывает стабильный, но умеренный рост при выходе на какую-то константу. Работает надежнее Pthreads, но ограничен ресурсами одного узла (Shared Memory) и накладными расходами библиотеки.MPI_Py (Синий): Почти не ускоряется.плюс огромные накладные расходы на  вызовы API, сопоставимо с временем самих вычислений это делает решение задачи при таком количестве точек очень слабо ускоряемой.

# Screencast
https://github.com/user-attachments/assets/122ff2b3-d618-4319-b24e-4eff6693becc

# Вывод

В ходе работы была проанализирована эффективность параллельного решения 1D уравнения Пуассона методами OpenMP, Pthreads и MPI в режиме Strong Scaling. Наилучшую производительность продемонстрировала реализация MPI на языке C, достигнувшая пикового ускорения около 4.8x благодаря компилируемой природе кода и отсутствию накладных расходов на управление памятью. Однако на использованной размерности сетки ($10^5$ элементов) после 48 процессов наблюдается снижение ускорения, так как время на сетевую коммуникацию начинает преобладать над полезными вычислениями, которые на одном ядре выполняются слишком быстро.Сравнение методов для общей памяти показало преимущество OpenMP перед Pthreads, так как ручная реализация барьеров в Pthreads создает критические накладные расходы при большом числе потоков, тогда как OpenMP управляет синхронизацией более эффективно. Версия на Python, несмотря на использование оптимизированных библиотек NumPy и mpi4py, показала минимальное ускорение, поскольку накладные расходы интерпретатора на организацию цикла и вызовы функций API перекрывают выигрыш от распараллеливания в задачах с большим количеством «легких» итераций. Для достижения линейного ускорения на 112 ядрах необходимо значительно увеличить размер решаемой задачи, чтобы нивелировать влияние сетевых задержек.
